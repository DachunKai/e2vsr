{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from basicsr.utils.event_utils import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = 'datasets/CED/HR//CED_additional_IR_filter/outdoor_shadow_1_infrared/events/000000.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(txt_file, delim_whitespace=True, header=None, names=['t', 'x', 'y', 'p'], dtype={'t': np.float64, 'x': np.int16, 'y': np.int16, 'p': np.int16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.from_numpy(data['t'].values)\n",
    "x = torch.from_numpy(data['x'].values)\n",
    "y = torch.from_numpy(data['y'].values)\n",
    "p = torch.from_numpy(data['p'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int16\n",
      "torch.int16\n",
      "torch.int16\n",
      "torch.Size([97])\n"
     ]
    }
   ],
   "source": [
    "print(x.dtype)\n",
    "print(y.dtype)\n",
    "print(p.dtype)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel = events_to_voxel_torch(x, y, t, p, 5, sensor_size=(260, 346))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxel_normalization(voxel):\n",
    "    \"\"\"\n",
    "        normalize the voxel same as https://arxiv.org/abs/1912.01584 Section 3.1\n",
    "        Params:\n",
    "            voxel: torch.Tensor, shape is [num_bins, H, W]\n",
    "        \n",
    "        return:\n",
    "            normalized voxel\n",
    "    \"\"\"\n",
    "    abs_voxel, _ = torch.sort(torch.abs(voxel).view(-1, 1).squeeze(1))\n",
    "    first_non_zero_idx = torch.nonzero(abs_voxel)[0].item()\n",
    "    non_zero_voxel = abs_voxel[first_non_zero_idx:]\n",
    "    norm_idx = math.floor(non_zero_voxel.shape[0] * 0.98)\n",
    "\n",
    "    ones = torch.ones_like(voxel)\n",
    "\n",
    "    normed_voxel = torch.where(voxel >= non_zero_voxel[norm_idx], ones, voxel)\n",
    "    normed_voxel = torch.where(voxel <= -non_zero_voxel[norm_idx], -ones, voxel)\n",
    "    normed_voxel = torch.where(torch.abs(voxel) < non_zero_voxel[norm_idx], voxel / non_zero_voxel[norm_idx], voxel)\n",
    "\n",
    "    return normed_voxel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_voxel = voxel_normalization(voxel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_voxel = normed_voxel.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 260, 346)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_voxel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'datasets/CED/CED_additional_IR_filter/outdoor_jumping_1_infrared'\n",
    "from glob import glob\n",
    "import os.path as osp\n",
    "events_paths = sorted(glob(osp.join(input, 'events/*.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/CED/CED_additional_IR_filter/outdoor_jumping_1_infrared/events/000000.txt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('datasets/CED/CED_additional_IR_filter/outdoor_jumping_1_infrared/events/000000',\n",
       " '.txt')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osp.splitext('datasets/CED/CED_additional_IR_filter/outdoor_jumping_1_infrared/events/000000.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('000001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_path_list = []\n",
    "h5_path_list = []\n",
    "folder_path = 'datasets/CED'\n",
    "for root, subdir, _ in os.walk(folder_path):\n",
    "    if 'images' in subdir:\n",
    "        root_path_list.append(root)\n",
    "        out = osp.join('datasets/CED_h5', osp.basename(root) + '.h5')\n",
    "        h5_path_list.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/CED_h5/simple_rabbits.h5'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_path_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/CED/CED_simple/simple_rabbits'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlist = []\n",
    "for root, h5 in zip(root_path_list, h5_path_list):\n",
    "    pathlist.append([root, h5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/CED/CED_simple/simple_rabbits', 'datasets/CED_h5/simple_rabbits.h5']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlist[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hdf5 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "path = 'datasets/CED_h5/calib_outdoor.h5'\n",
    "file = h5py.File(path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1555332068.1933038'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['images/000000'].attrs['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.keys()\n",
    "print(file['events'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlist = ['datasets/CED/CED_simple/simple_rabbits', 'datasets/CED_h5/simple_rabbits.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/tqdm-4.64.1-py3.7.egg/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from basicsr.utils.hdf5_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package simple_rabbits.h5 finished.\n"
     ]
    }
   ],
   "source": [
    "write_h5_worker(pathlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/tqdm-4.64.1-py3.7.egg/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from basicsr.utils import scandir\n",
    "import os\n",
    "from glob import glob\n",
    "import  os.path as osp\n",
    "\n",
    "h5_path = glob(osp.join('datasets/CED_h5', '*.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "file = h5py.File('datasets/CED_h5/driving_city_5.h5', 'r')\n",
    "len(file['voxel'].keys())\n",
    "file['voxel/004318'].attrs['is_empty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/001765.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/001766.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/001767.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/001768.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/001769.txt is empty, will write zero voxel\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2078089/3787823068.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'datasets/CED_h5/HR/driving_city_sun_2.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mwrite_h5_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/e2vsr/basicsr/utils/hdf5_util.py\u001b[0m in \u001b[0;36mwrite_h5_worker\u001b[0;34m(pathlist)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_paths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackage_voxel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;31m# Step 4: Add meta_info to h5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/e2vsr/basicsr/utils/hdf5_util.py\u001b[0m in \u001b[0;36mpackage_voxel\u001b[0;34m(self, txt_file, bins)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mvoxel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevents_to_voxel_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensor_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m260\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m346\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# print(\"voxel.shape: \", voxel.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mnormed_voxel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoxel_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoxel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mnp_voxel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormed_voxel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/e2vsr/basicsr/utils/event_utils.py\u001b[0m in \u001b[0;36mvoxel_normalization\u001b[0;34m(voxel)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoxel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvoxel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0mabs_voxel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoxel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;31m# print(\"abs_voxel.shape: \", abs_voxel.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mfirst_non_zero_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_voxel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from basicsr.utils.hdf5_util import write_h5_worker\n",
    "pathlist = [\n",
    "    'datasets/CED/HR/CED_driving/driving_city_sun_2',\n",
    "    'datasets/CED_h5/HR/driving_city_sun_2.h5'\n",
    "]\n",
    "write_h5_worker(pathlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package people_dynamic_wave.h5 finished.\n"
     ]
    }
   ],
   "source": [
    "from basicsr.utils.hdf5_util import write_h5_worker\n",
    "pathlist = [\n",
    "    'datasets/CED/HR/CED_people/people_dynamic_wave',\n",
    "    'datasets/CED_h5/HR/people_dynamic_wave.h5'\n",
    "]\n",
    "write_h5_worker(pathlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/tqdm-4.64.1-py3.7.egg/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package simple_jenga_destroy.h5 finished.\n"
     ]
    }
   ],
   "source": [
    "from basicsr.utils.hdf5_util import write_h5_worker\n",
    "pathlist = [\n",
    "    'datasets/CED/HR/CED_simple/simple_jenga_destroy',\n",
    "    'datasets/CED_h5/HR/simple_jenga_destroy.h5'\n",
    "]\n",
    "write_h5_worker(pathlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "txt_file = 'datasets/CED/CED_driving/driving_city_5/events/004318.txt'\n",
    "data = pd.read_csv(txt_file, delim_whitespace=True, header=None, names=['t', 'x', 'y', 'p'], dtype={'t': np.float64, 'x': np.int16, 'y': np.int16, 'p': np.int16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((5, 260, 346))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 260, 346)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "root_path = 'datasets/CED_h5/HR'\n",
    "from glob import glob\n",
    "import os\n",
    "import os.path as osp \n",
    "h5_list = glob(osp.join(root_path, \"*.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path_list = []\n",
    "h5_path_list = []\n",
    "folder_path = 'datasets/CED/HR'\n",
    "for root, subdir, _ in os.walk(folder_path):\n",
    "    if 'images' in subdir:\n",
    "        root_path_list.append(root)\n",
    "        out = osp.join('datasets/CED_h5/HR', osp.basename(root) + '.h5')\n",
    "        h5_path_list.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/CED/HR/CED_simple/simple_rabbits', 'datasets/CED_h5/HR/simple_rabbits.h5']\n"
     ]
    }
   ],
   "source": [
    "total_path = []\n",
    "for root, h5 in zip(root_path_list, h5_path_list):\n",
    "    total_path.append([root, h5])\n",
    "print(total_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "print(len(total_path))\n",
    "for f in total_path:\n",
    "    len_ori = len(glob(osp.join(f[0], 'events', \"*.txt\")))\n",
    "\n",
    "    file = h5py.File(f[1], 'r')\n",
    "    event_num1 = len(file['voxels'].keys())\n",
    "    # if 'voxels' in file.keys():\n",
    "    #     event_num2 = len(file['voxels'].keys())\n",
    "    # else:\n",
    "    #     event_num2 = 0\n",
    "    event_num = event_num1\n",
    "    if len_ori != event_num:\n",
    "        print(f[1], len_ori, event_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932\n"
     ]
    }
   ],
   "source": [
    "file = h5py.File('datasets/CED_h5/simple_fruit_fast.h5', 'r')\n",
    "print(len(file['voxel'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cp HR/timestamp.txt to LR/timestamp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_path = 'datasets/CED/HR/'\n",
    "LR_path = 'datasets/CED/LRx4'\n",
    "for root, subdir, files in os.walk(HR_path):\n",
    "    if 'timestamp.txt' in files:\n",
    "        folder = root.split(HR_path)[1]\n",
    "        src = osp.join(HR_path, folder, 'timestamp.txt')\n",
    "        dst = osp.join(LR_path, folder)\n",
    "        os.system(f\"cp -rf {src} {dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get hr voxel and use bicubic to downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "folder_path = 'datasets/CED/LR'\n",
    "root_path_list = []\n",
    "h5_hr_path_list = []\n",
    "h5_lr_path_list = []\n",
    "for root, subdir, _ in os.walk(folder_path):\n",
    "    if 'images' in subdir:\n",
    "        root_path_list.append(root)\n",
    "        h5_hr_path_list.append(osp.join('datasets/CED_h5/HR', osp.basename(root) + '.h5'))\n",
    "        h5_lr_path_list.append(osp.join('datasets/CED_h5/LR', osp.basename(root) + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/CED_h5/HR/simple_rabbits.h5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_hr_path_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "file = h5py.File(h5_hr_path_list[0], 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['duration', 'num_events', 'num_imgs', 'sensor_resolution', 't0', 'tk']>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['voxels/000000'].attrs['is_empty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 260, 346)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxel_ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/tqdm-4.64.1-py3.7.egg/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./scripts/data_preparation')\n",
    "from scripts.data_preparation.pyResize import DownSample, UpSample, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_lr = imresize(voxel_ex, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 130, 346)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxel_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "voxel_tensor = torch.from_numpy(voxel_ex).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    }
   ],
   "source": [
    "lr = torch.nn.functional.interpolate(input = voxel_tensor, scale_factor = 0.5, mode = 'bicubic').squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 130, 173)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test lr h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package simple_carpet.h5 finished.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from basicsr.utils.hdf5_util import *\n",
    "\n",
    "pathlist = [\n",
    "    'datasets/CED/LR/CED_simple/simple_carpet',\n",
    "    'datasets/CED_h5/HR/simple_carpet.h5',\n",
    "    'datasets/CED_h5/LR/simple_carpet.h5'\n",
    "]\n",
    "\n",
    "write_lr_h5_worker(pathlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['duration', 'num_events', 'num_imgs', 'sensor_resolution', 't0', 'tk']>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "file = h5py.File('datasets/CED_h5/HR/driving_city_4.h5', 'r')\n",
    "file.attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 260, 346)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['voxels/000000'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17514"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.attrs['num_events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people/people_dynamic_wave/images 759 (260,346,3)', 'indoors/indoors_foosball_2/images 269 (260,346,3)', 'simple/simple_wires_2/images 552 (260,346,3)', 'people/people_dynamic_dancing/images 1175 (260,346,3)', 'people/people_dynamic_jumping/images 792 (260,346,3)', 'simple/simple_fruit_fast/images 933 (260,346,3)', 'additional_IR_filter/outdoor_jumping_infrared_2/images 665 (260,346,3)', 'simple/simple_carpet_fast/images 602 (260,346,3)', 'people/people_dynamic_armroll/images 792 (260,346,3)', 'indoors/indoors_kitchen_2/images 635 (260,346,3)', 'people/people_dynamic_sitting/images 1075 (260,346,3)']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "txt_path = '/home/kaidc/code/e2vsr/basicsr/data/meta_info/CED_tmp.txt'\n",
    "\n",
    "with open(txt_path, 'r') as f:\n",
    "    str_temp = f.read().splitlines()\n",
    "    print(str_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people_dynamic_wave 759\n"
     ]
    }
   ],
   "source": [
    "to_write = str_temp[0].split(' ')[0].split('/')[1] + ' ' + str_temp[0].split(' ')[1]\n",
    "print(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = '/home/kaidc/code/e2vsr/basicsr/data/meta_info/CED_h5_test.txt'\n",
    "\n",
    "with open(txt_path, 'w') as f:\n",
    "    for i in range(len(str_temp)):\n",
    "        to_write = str_temp[i].split(' ')[0].split('/')[1] + '.h5 ' + str_temp[i].split(' ')[1] + '\\n'\n",
    "        f.write(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = '/home/kaidc/code/e2vsr/basicsr/data/meta_info/CED_h5_train_2.txt'\n",
    "\n",
    "with open(txt_path, 'w') as f:\n",
    "    for i in range(len(str_temp)):\n",
    "        f.write(str_temp[i].split(' ')[0].split('/')[1] + '.h5\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "file = 'basicsr/data/meta_info/CED_h5_train.txt'\n",
    "\n",
    "keys = []\n",
    "frame_num = {}\n",
    "with open(file, 'r') as fin:\n",
    "    for line in fin:\n",
    "        name, num = line.split(' ')\n",
    "        keys.extend([f'{name}/{i:06d}' for i in range(int(num))])\n",
    "        print(osp.dirname(\"simple_flowers_infrared.h5\"))\n",
    "        frame_num[osp.dirname(name)] = num.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_num.get('simple_flowers_infrared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_num = {}\n",
    "name = 'simple'\n",
    "num = '10'\n",
    "frame_num[name] = num\n",
    "type(frame_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('datasets/CED_h5/HR/people_dynamic_wave.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['images', 'voxels']>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = file['images/000003'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_path = 'datasets/CED/HR/CED_people/people_dynamic_wave/images/000000.png'\n",
    "img_np = cv2.imread(np_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img == img_np).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('tmp/img.png', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "path = '/home/kaidc/data/VSR/CED_lmdb/train_sharp_with_val.lmdb'\n",
    "\n",
    "env = lmdb.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn = env.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = txn.get(str('people/people_dynamic_wave/images/000000').encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basicsr.utils.img_util import imfrombytes\n",
    "img2 = imfrombytes(img2, float32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99607843"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.max(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('tmp/img2.png', img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "file = h5py.File('datasets/CED_h5/Voxel_3/LRx4/driving_city_sun_2.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['images', 'voxels']>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['duration', 'num_events', 'num_imgs', 'sensor_resolution', 't0', 'tk']>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 65, 86)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['voxels/003940'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 86, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['images/000000'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basicsr.utils.hdf5_util import write_h5_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlist = [\n",
    "    'datasets/CED/HR/CED_driving/driving_city_sun_2',\n",
    "    'datasets/CED_h5/Voxel_3/HR/driving_city_sun_2.h5'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/001765.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/001766.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/001767.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/001768.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/001769.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/003940.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/003941.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/003942.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/003943.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/005127.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/005128.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/005129.txt is empty, will write zero voxel\n",
      "datasets/CED/HR/CED_driving/driving_city_sun_2/events/005130.txt is empty, will write zero voxel\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3261376/3337437573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_h5_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/e2vsr/basicsr/utils/hdf5_util.py\u001b[0m in \u001b[0;36mwrite_h5_worker\u001b[0;34m(pathlist)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_paths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackage_voxel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# Step 4: Add meta_info to h5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/e2vsr/basicsr/utils/hdf5_util.py\u001b[0m in \u001b[0;36mpackage_voxel\u001b[0;34m(self, txt_file, bins)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{txt_file} does not exist!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mevent_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/e2vsr/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/e2vsr/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/e2vsr/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/e2vsr/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/e2vsr/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/e2vsr/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/e2vsr/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/e2vsr/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/e2vsr/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/e2vsr/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "write_h5_worker(pathlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test mutual attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "import numbers\n",
    "from torch import nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "def to_4d(x,h,w):\n",
    "    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
    "\n",
    "class BiasFree_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(BiasFree_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return x / torch.sqrt(sigma+1e-5) * self.weight\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if LayerNorm_type =='BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)\n",
    "\n",
    "\n",
    "class Mutual_Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias):\n",
    "        super(Mutual_Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        self.k = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        self.v = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        assert x.shape == y.shape, 'The shape of feature maps from image and event branch are not equal!'\n",
    "\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        q = self.q(x) # image\n",
    "        k = self.k(y) # event\n",
    "        v = self.v(y) # event\n",
    "\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v)\n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "        out = self.project_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "path = 'datasets/CED_h5/Voxel_3/LRx4/driving_city_sun_2.h5'\n",
    "\n",
    "file = h5py.File(path, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['voxels/000000'].attrs['is_empty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn((1, 3, 64, 64))\n",
    "event = torch.zeros((1, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = Mutual_Attention(3, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = attn(img, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<SlowConv2DBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# attn = torch.randn((1, 1, 3, 3)).cuda()\n",
    "attn = torch.zeros((1, 1, 3, 3)).cuda()\n",
    "print(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = attn.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3333, 0.3333, 0.3333],\n",
       "          [0.3333, 0.3333, 0.3333],\n",
       "          [0.3333, 0.3333, 0.3333]]]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/CED_2/HR/CED_simple/simple_rabbits\n",
      "datasets/CED_2/HR/CED_simple/simple_objects_dynamic\n",
      "datasets/CED_2/HR/CED_simple/simple_color_keyboard_2\n",
      "datasets/CED_2/HR/CED_simple/simple_objects\n",
      "datasets/CED_2/HR/CED_simple/simple_wires_1\n",
      "datasets/CED_2/HR/CED_simple/simple_color_keyboard_3\n"
     ]
    }
   ],
   "source": [
    "root_path = 'datasets/CED_2/HR/'\n",
    "\n",
    "cnt = 0\n",
    "for root, subdir, _ in os.walk(root_path):\n",
    "    if 'events' in subdir:\n",
    "        cnt += 1\n",
    "        print(root)\n",
    "    if cnt > 5:\n",
    "        break\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file = open('test.txt')\n",
    "lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0\\n', '1\\n', '2\\n', '3\\n', '4\\n', '5\\n', '6']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_list = [0, 1, 2]\n",
    "for i in empty_list:\n",
    "    del lines[i]\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "def process_dir(list):\n",
    "\n",
    "    events_folder = osp.join(list[0], 'events')\n",
    "    images_folder = osp.join(list[0], 'images')\n",
    "    empty_list = []\n",
    "    event_list = sorted(glob(osp.join(events_folder, '*.txt')))\n",
    "    for idx in range(len(event_list)):\n",
    "        if not osp.getsize(event_list[idx]):\n",
    "            empty_list.append(idx)\n",
    "            os.remove(event_list[idx])\n",
    "            os.remove(osp.join(images_folder, f'{(idx+1):06d}.png'))\n",
    "\n",
    "    with open(osp.join(list[0], 'timestamp.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in reversed(empty_list):\n",
    "            del lines[i+1]\n",
    "        f = open(osp.join(list[0], 'timestamp.txt'), 'w')\n",
    "        f.writelines(lines)\n",
    "        f.close()\n",
    "\n",
    "    img_list = sorted(glob(osp.join(images_folder, '*.png')))\n",
    "    for i in range(len(img_list)):\n",
    "        old_name = img_list[i]\n",
    "        new_name = osp.join(images_folder, f'{i:06d}.png')\n",
    "        os.rename(old_name, new_name)\n",
    "\n",
    "    event_list = sorted(glob(osp.join(events_folder, '*.txt')))\n",
    "    for i in range(len(event_list)):\n",
    "        old_name = event_list[i]\n",
    "        new_name = osp.join(events_folder, f'{i:06d}.txt')\n",
    "        os.rename(old_name, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = ['datasets/CED_2/HR/CED_driving/driving_city_sun_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dir(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/CED_2/HR/CED_driving/driving_city_sun_2'\n",
    "events_folder = osp.join(path, 'events')\n",
    "event_list = sorted(glob(osp.join(events_folder, '*.txt')))\n",
    "for idx in range(len(event_list)):\n",
    "    if not osp.getsize(event_list[idx]):\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_path = 'datasets/CED_2/HR'\n",
    "\n",
    "pathlist = []\n",
    "for root, subdir, _ in os.walk(root_path):\n",
    "    if 'images' in subdir:\n",
    "        pathlist.append(root)\n",
    "\n",
    "print(len(pathlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/CED_2/HR/CED_simple/simple_rabbits'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "#input list is [''datasets/CED/HR/CED_simple/simple_rabbits', 'datasets/CED_2/LRx4/CED_simple/simple_rabbits']\n",
    "def process_dir(list):\n",
    "    print(list[0])\n",
    "\n",
    "    events_folder = osp.join(list[0], 'events')\n",
    "    images_folder = osp.join(list[1], 'images')\n",
    "    empty_list = []\n",
    "    event_list = sorted(glob(osp.join(events_folder, '*.txt')))\n",
    "    for idx in range(len(event_list)):\n",
    "        if not osp.getsize(event_list[idx]):\n",
    "            empty_list.append(idx)\n",
    "            # os.remove(event_list[idx])\n",
    "            # print(f\"remove empty file {event_list[idx]}\")\n",
    "            os.remove(osp.join(images_folder, f'{(idx+1):06d}.png'))\n",
    "            print(f\"remove empty file {event_list[idx]}\")\n",
    "\n",
    "    with open(osp.join(list[1], 'timestamp.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in reversed(empty_list):\n",
    "            del lines[i+1]\n",
    "        f = open(osp.join(list[1], 'timestamp.txt'), 'w')\n",
    "        f.writelines(lines)\n",
    "        f.close()\n",
    "\n",
    "    img_list = sorted(glob(osp.join(images_folder, '*.png')))\n",
    "    for i in range(len(img_list)):\n",
    "        old_name = img_list[i]\n",
    "        new_name = osp.join(images_folder, f'{i:06d}.png')\n",
    "        os.rename(old_name, new_name)\n",
    "\n",
    "    # event_list = sorted(glob(osp.join(events_folder, '*.txt')))\n",
    "    # for i in range(len(event_list)):\n",
    "    #     old_name = event_list[i]\n",
    "    #     new_name = osp.join(events_folder, f'{i:06d}.txt')\n",
    "    #     os.rename(old_name, new_name)\n",
    "\n",
    "    out_message = list[1].split('/')[-2] + '/' + list[1].split('/')[-1]\n",
    "    print(f\"Process {out_message} finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'datasets/CED_2/LRx4'\n",
    "\n",
    "pathlist = []\n",
    "for root, subdir, _ in os.walk(root_path):\n",
    "    if 'images' in subdir:\n",
    "        former = osp.join('datasets/CED', root.split('/CED_2/LRx4/')[1])\n",
    "        pathlist.append([former, root])\n",
    "\n",
    "T1 = time.time()\n",
    "\n",
    "pool = Pool(40)\n",
    "pool.map(process_dir, pathlist)\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "root_path = 'datasets/CED_2/LRx4'\n",
    "\n",
    "pathlist = []\n",
    "for root, subdir, _ in os.walk(root_path):\n",
    "    if 'images' in subdir:\n",
    "        former = osp.join('datasets/CED/HR', root.split('/CED_2/LRx4/')[1])\n",
    "        pathlist.append([former, root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/CED/HR/CED_simple/simple_rabbits',\n",
       " 'datasets/CED_2/LRx4/CED_simple/simple_rabbits']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "file = h5py.File('datasets/CED_2_h5/Voxel_3/HR/simple_fruit_fast.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 260, 346)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['voxels/000000'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make meta_info CED_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path as osp\n",
    "\n",
    "root_path = 'datasets/CED_3/HR'\n",
    "wr_file = '/home/kaidc/code/e2vsr/basicsr/data/meta_info/mata_info_CED_3_h5_train.txt'\n",
    "\n",
    "with open(wr_file, 'w') as f:\n",
    "    for root, subdir, _ in os.walk(root_path):\n",
    "        if 'images' in subdir:\n",
    "            length = str(len(glob.glob(osp.join(root, 'images', '*.png'))))\n",
    "            wr_str = osp.basename(root) + '.h5' + ' ' + length + '\\n'\n",
    "            f.write(wr_str)\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "file_path = 'datasets/CED_3_h5/Voxel_3/HR/indoors_foosball_1.h5'\n",
    "\n",
    "file = h5py.File(file_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(file['voxels/000001'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(file.attrs['num_events']):\n",
    "    if np.min(file[f'voxels/{i:06d}'][:]) < 0:\n",
    "        print(f\"{i:06d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(temp == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(temp).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from basicsr.utils.hdf5_util import hdf5_packager\n",
    "test_h5 = hdf5_packager('./temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = 'datasets/CED/HR/CED_additional_IR_filter/outdoor_shadow_1_infrared/events/000000.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "test_h5.package_voxel(txt_file, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "file_2 = h5py.File('./temp.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['voxels']>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_2.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['000144']>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_2['voxels'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = file_2['voxels/000144'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(txt_file, delim_whitespace=True, header=None, names=['t', 'x', 'y', 'p'], dtype={'t': np.float64, 'x': np.int16, 'y': np.int16, 'p': np.int16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.555342e+09</td>\n",
       "      <td>156</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              t    x    y  p\n",
       "0  1.555342e+09  156  217  1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.from_numpy(data['t'].values)\n",
    "x = torch.from_numpy(data['x'].values)\n",
    "y = torch.from_numpy(data['y'].values)\n",
    "p = torch.from_numpy(data['p'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([156], dtype=torch.int16) tensor([217], dtype=torch.int16) tensor([1.5553e+09], dtype=torch.float64) tensor([1], dtype=torch.int16) 3\n"
     ]
    }
   ],
   "source": [
    "from basicsr.utils.event_utils import *\n",
    "bins = 3\n",
    "print(x, y, t, p, bins)\n",
    "voxel = events_to_voxel_torch(x, y, t, p, bins, device=None, sensor_size=(260, 346))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dt = t[-1]-t[0]\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 3\n",
    "t_norm = (t-t[0])/dt*(B-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan], dtype=torch.float64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.log import error\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "#input list is [''datasets/CED_3/HR/CED_simple/simple_rabbits']\n",
    "def process_dir(list):\n",
    "    print(list[0])\n",
    "\n",
    "    events_folder = osp.join(list[0], 'events')\n",
    "    images_folder = osp.join(list[0], 'images')\n",
    "    error_list = []\n",
    "    event_list = sorted(glob(osp.join(events_folder, '*.txt')))\n",
    "\n",
    "    for idx in range(len(event_list)):\n",
    "        with open(event_list[idx], 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) == 1:\n",
    "\n",
    "                if idx == (len(event_list) - 1):\n",
    "                    error_list.append(idx)\n",
    "                    os.remove(event_list[idx])\n",
    "                    os.remove(osp.join(images_folder, f'{(idx+1):06d}.png'))\n",
    "                    print(\"error the last one idx: \", idx)\n",
    "                    print(f\"remove event file {event_list[idx]}\")\n",
    "                    break\n",
    "\n",
    "                with open(event_list[idx+1], 'r+') as f2:\n",
    "                    content = f2.read()\n",
    "                    f2.seek(0, 0)\n",
    "                    f2.write(lines[0]+content)\n",
    "\n",
    "                error_list.append(idx)\n",
    "                print(\"error one idx: \", idx)\n",
    "\n",
    "                os.remove(event_list[idx])\n",
    "                os.remove(osp.join(images_folder, f'{(idx+1):06d}.png'))\n",
    "                print(f\"remove event file {event_list[idx]}\")\n",
    "\n",
    "            elif len(lines) == 0:\n",
    "                error_list.append(idx)\n",
    "                os.remove(event_list[idx])\n",
    "                print(f\"remove empty file {event_list[idx]}\")\n",
    "                os.remove(osp.join(images_folder, f'{(idx+1):06d}.png'))\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    with open(osp.join(list[0], 'timestamp.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in reversed(sorted(error_list)):\n",
    "            del lines[i+1]\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    f = open(osp.join(list[0], 'timestamp.txt'), 'w')\n",
    "    f.writelines(lines)\n",
    "    f.close()\n",
    "\n",
    "    img_list = sorted(glob(osp.join(images_folder, '*.png')))\n",
    "    for i in range(len(img_list)):\n",
    "        old_name = img_list[i]\n",
    "        new_name = osp.join(images_folder, f'{i:06d}.png')\n",
    "        os.rename(old_name, new_name)\n",
    "\n",
    "    event_list = sorted(glob(osp.join(events_folder, '*.txt')))\n",
    "    for i in range(len(event_list)):\n",
    "        old_name = event_list[i]\n",
    "        new_name = osp.join(events_folder, f'{i:06d}.txt')\n",
    "        os.rename(old_name, new_name)\n",
    "\n",
    "    out_message = list[0].split('/')[-2] + '/' + list[0].split('/')[-1]\n",
    "    print(f\"Process {out_message} finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/CED_3/HR/CED_additional_IR_filter/outdoor_shadow_1_infrared\n",
      "error one idx:  144\n",
      "remove event file datasets/CED_3/HR/CED_additional_IR_filter/outdoor_shadow_1_infrared/events/000144.txt\n",
      "remove empty file datasets/CED_3/HR/CED_additional_IR_filter/outdoor_shadow_1_infrared/events/000161.txt\n",
      "error one idx:  165\n",
      "remove event file datasets/CED_3/HR/CED_additional_IR_filter/outdoor_shadow_1_infrared/events/000165.txt\n",
      "error the last one idx:  573\n",
      "remove event file datasets/CED_3/HR/CED_additional_IR_filter/outdoor_shadow_1_infrared/events/000573.txt\n",
      "Process CED_additional_IR_filter/outdoor_shadow_1_infrared finished.\n"
     ]
    }
   ],
   "source": [
    "process_dir(['datasets/CED_3/HR/CED_additional_IR_filter/outdoor_shadow_1_infrared'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'datasets/CED_3/HR/CED_driving/driving_city_sun_2/events/001765.txt'\n",
    "with open(file, 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'datasets/CED_3/HR'\n",
    "\n",
    "pathlist = []\n",
    "for root, subdir, _ in os.walk(root_path):\n",
    "    if 'images' in subdir:\n",
    "        # former = osp.join('datasets/CED/HR', root.split('/CED_2/LRx2/')[1])\n",
    "        pathlist.append([root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['datasets/CED_3/HR/CED_simple/simple_rabbits'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_objects_dynamic'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_color_keyboard_2'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_objects'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_wires_1'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_color_keyboard_3'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_jenga_1'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_color_keyboard_1'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_carpet'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_fruit_fast'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_carpet_fast'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_flowers_infrared'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_wires_2'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_jenga_2'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_jenga_destroy'],\n",
       " ['datasets/CED_3/HR/CED_simple/simple_fruit'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_foosball_1'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_window_autoexposure'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_corridor'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_kitchen_1'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_kitchen_fast'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_office'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_flying_room'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_foosball_3'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_window'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_dark_25ms'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_very_dark_250ms'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_dark_100ms'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_very_dark_25ms'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_foosball_2'],\n",
       " ['datasets/CED_3/HR/CED_indoors/indoors_kitchen_2'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_fluorescent_infrared'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_low'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_outdoor_density'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_outdoor'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_fluorescent_dynamic'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_fluorescent_density_infrared'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_fluorescent'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_outdoor_infrared'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_fluorescent_density'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_outdoor_hdr_infrared'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_outdoor_dynamic'],\n",
       " ['datasets/CED_3/HR/CED_calibration/calib_low_density'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_city_4'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_tunnel'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_city_sun_2'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_country_sun_2'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_city_3'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_country'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_country_sun_1'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_city_2'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_city_5'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_city_1'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_city_sun_1'],\n",
       " ['datasets/CED_3/HR/CED_driving/driving_tunnel_sun'],\n",
       " ['datasets/CED_3/HR/CED_additional_IR_filter/outdoor_jumping_1_infrared'],\n",
       " ['datasets/CED_3/HR/CED_additional_IR_filter/outdoor_shadow_1_infrared'],\n",
       " ['datasets/CED_3/HR/CED_additional_IR_filter/outdoor_jumping_infrared_2'],\n",
       " ['datasets/CED_3/HR/CED_additional_IR_filter/outdoor_shadow_density_infrared'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_wave'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_wave_clockwise'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_armroll'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_air_guitar'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_clap'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_dancing_multiple_2'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_dancing'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_jumping'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_jumping'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_wave_clockwise'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_wave'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_jogging'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_sitting'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_wave_counterclockwise'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_sitting'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_dancing_multiple'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_wave_counterclockwise'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_clap'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_jogging'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_arm_roll'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_dancing_multiple_1'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_air_guitar'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_dancing'],\n",
       " ['datasets/CED_3/HR/CED_people/people_dynamic_selfie'],\n",
       " ['datasets/CED_3/HR/CED_people/people_static_dancing_multiple_3']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.log import error\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "#input list is [''datasets/CED_3/HR/CED_simple/simple_rabbits']\n",
    "def process_dir(list):\n",
    "    # print(list[0])\n",
    "\n",
    "    events_folder = osp.join(list[0], 'events')\n",
    "    images_folder = osp.join(list[0], 'images')\n",
    "    # error_list = []\n",
    "    event_list = sorted(glob(osp.join(events_folder, '*.txt')))\n",
    "    img_list = sorted(glob(osp.join(images_folder, \"*.png\")))\n",
    "    f = open(osp.join(list[0], 'timestamp.txt'), 'r')\n",
    "    lines = f.readlines()\n",
    "    assert len(event_list) == len(img_list) - 1 == len(lines) - 1, f\"check foloder\"\n",
    "\n",
    "    for idx in range(len(event_list)):\n",
    "        with open(event_list[idx], 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) == 1:\n",
    "\n",
    "                # if idx == (len(event_list) - 1):\n",
    "                #     error_list.append(idx)\n",
    "                #     os.remove(event_list[idx])\n",
    "                #     os.remove(osp.join(images_folder, f'{(idx+1):06d}.png'))\n",
    "                #     print(\"error the last one idx: \", idx)\n",
    "                #     print(f\"remove event file {event_list[idx]}\")\n",
    "                #     break\n",
    "\n",
    "                # with open(event_list[idx+1], 'r+') as f2:\n",
    "                    # content = f2.read()\n",
    "                    # f2.seek(0, 0)\n",
    "                    # f2.write(lines[0]+content)\n",
    "\n",
    "                # error_list.append(idx)\n",
    "                print(\"error one idx: \", event_list[idx])\n",
    "\n",
    "                # os.remove(event_list[idx])\n",
    "                # os.remove(osp.join(images_folder, f'{(idx+1):06d}.png'))\n",
    "                # print(f\"remove event file {event_list[idx]}\")\n",
    "\n",
    "            elif len(lines) == 0:\n",
    "                # error_list.append(idx)\n",
    "                # os.remove(event_list[idx])\n",
    "                print(\"empty one idx: \", event_list[idx])\n",
    "                # print(f\"remove empty file {event_list[idx]}\")\n",
    "                # os.remove(osp.join(images_folder, f'{(idx+1):06d}.png'))\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # with open(osp.join(list[0], 'timestamp.txt'), 'r') as f:\n",
    "    #     lines = f.readlines()\n",
    "    #     for i in reversed(sorted(error_list)):\n",
    "    #         del lines[i+1]\n",
    "\n",
    "    # f.close()\n",
    "\n",
    "    # f = open(osp.join(list[0], 'timestamp.txt'), 'w')\n",
    "    # f.writelines(lines)\n",
    "    # f.close()\n",
    "\n",
    "    # img_list = sorted(glob(osp.join(images_folder, '*.png')))\n",
    "    # for i in range(len(img_list)):\n",
    "    #     old_name = img_list[i]\n",
    "    #     new_name = osp.join(images_folder, f'{i:06d}.png')\n",
    "    #     os.rename(old_name, new_name)\n",
    "\n",
    "    # event_list = sorted(glob(osp.join(events_folder, '*.txt')))\n",
    "    # for i in range(len(event_list)):\n",
    "    #     old_name = event_list[i]\n",
    "    #     new_name = osp.join(events_folder, f'{i:06d}.txt')\n",
    "    #     os.rename(old_name, new_name)\n",
    "\n",
    "    # out_message = list[0].split('/')[-2] + '/' + list[0].split('/')[-1]\n",
    "    # print(f\"Process {out_message} finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'datasets/CED_3/HR'\n",
    "\n",
    "pathlist = []\n",
    "for root, subdir, _ in os.walk(root_path):\n",
    "    if 'images' in subdir:\n",
    "        # former = osp.join('datasets/CED/HR', root.split('/CED_2/LRx2/')[1])\n",
    "        pathlist.append([root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(40)\n",
    "pool.map(process_dir, pathlist)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "root_path = 'datasets/CED_3/LRx4'\n",
    "\n",
    "pathlist = []\n",
    "for root, subdir, _ in os.walk(root_path):\n",
    "    if 'images' in subdir:\n",
    "        former = osp.join('datasets/CED/HR', root.split('/CED_3/LRx4/')[1])\n",
    "        pathlist.append([former, root])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/CED/HR/CED_simple/simple_rabbits',\n",
       " 'datasets/CED_3/LRx4/CED_simple/simple_rabbits']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_0_to_neg(txt_file):\n",
    "    to_write_list = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            if line[-1] == '0':\n",
    "                line[-1] = '-1'\n",
    "            to_write = line[0] + ' ' + line[1]+ ' ' + line[2] + ' ' + line[-1] + '\\n'\n",
    "            to_write_list.append(to_write)\n",
    "    f.close()\n",
    "\n",
    "    f = open(txt_file, 'w')\n",
    "    f.writelines(to_write_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = '000000.txt'\n",
    "change_0_to_neg(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "file = h5py.File('1-4-f-200.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['images', 'masks', 'sharp_images', 'voxels']>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "EF_im = file['images/image000000000'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EF_vo = file['voxels/voxel000000000'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(EF_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.log import error\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "root_path = 'datasets/CED_3/HR'\n",
    "\n",
    "dir_list = []\n",
    "for root, subdir, _ in os.walk(root_path):\n",
    "    if 'events' in subdir:\n",
    "        dir_list.append([osp.join(root, 'events')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/CED_3/HR/CED_simple/simple_rabbits/events']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir(list):\n",
    "    event_list = sorted(glob(osp.join(list[0], '*.txt')))\n",
    "    for txt_file in event_list:\n",
    "        change_0_to_neg(txt_file)\n",
    "\n",
    "    out_message = list[0].split('/')[-3] + '/' + list[0].split('/')[-1]\n",
    "    print(f\"Process {out_message} finished.\")\n",
    "\n",
    "def change_0_to_neg(txt_file):\n",
    "    to_write_list = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            if line[-1] == '0':\n",
    "                line[-1] = '-1'\n",
    "            to_write = line[0] + ' ' + line[1]+ ' ' + line[2] + ' ' + line[-1] + '\\n'\n",
    "            to_write_list.append(to_write)\n",
    "    f.close()\n",
    "\n",
    "    f = open(txt_file, 'w')\n",
    "    f.writelines(to_write_list)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/CED_3/HR/CED_simple/simple_rabbits/events']\n",
      "Process CED_simple/events finished.\n"
     ]
    }
   ],
   "source": [
    "print(dir_list[0])\n",
    "process_dir(dir_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "folder_path = 'datasets/CED_2/HR'\n",
    "root_path_list = []\n",
    "h5_path_list = []\n",
    "for root, subdir, _ in os.walk(folder_path):\n",
    "    if 'images' in subdir:\n",
    "        root_path_list.append(root)\n",
    "        out = osp.join('datasets/CED_2_h5/Voxel_3/HR', osp.basename(root) + '.h5')\n",
    "        h5_path_list.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/CED_2/HR/CED_simple/simple_rabbits'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/CED_3_h5/Voxel_3/HR/simple_rabbits.h5'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_path_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basicsr.utils.hdf5_util import *\n",
    "\n",
    "test_list = ['datasets/CED_2/HR/CED_simple/simple_rabbits', 'datasets/CED_3_h5/Voxel_3/HR/driving_tunnel.h5']\n",
    "\n",
    "write_h5_worker(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "file = h5py.File('datasets/CED_3_h5/Voxel_3/HR/driving_tunnel.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = file['voxels/000000'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0646965503692627\n",
      "1.2698324918746948\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.min(np_arr))\n",
    "print(np.max(np_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaidc/miniconda3/envs/e2vsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from basicsr.utils.hdf5_util import hdf5_packager\n",
    "\n",
    "file = hdf5_packager('./temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = 'datasets/CED_3/HR/CED_driving/driving_tunnel/events/000000.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.package_voxel(txt_file, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vimeo&vid4 process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## meta_info_vimeo_h5_train.txt\n",
    "train_txt = 'datasets/vimeo_septuplet_h5/sep_trainlist.txt'\n",
    "meta_file = 'basicsr/data/meta_info/meta_info_vimeo_h5_train.txt'\n",
    "\n",
    "with open(train_txt, 'r') as f1:\n",
    "    line_list = f1.read().splitlines()\n",
    "f1.close()\n",
    "\n",
    "with open(meta_file, 'w') as f2:\n",
    "    for line in line_list:\n",
    "        to_wrt = line.replace('/', '_') + '.h5' + ' 13' + '\\n'\n",
    "        f2.write(to_wrt)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## meta_info_vimeo_h5_test.txt\n",
    "test_txt = 'datasets/vimeo_septuplet_h5/sep_testlist.txt'\n",
    "meta_file = 'basicsr/data/meta_info/meta_info_vimeo_h5_test.txt'\n",
    "\n",
    "with open(test_txt, 'r') as f1:\n",
    "    line_list = f1.read().splitlines()\n",
    "f1.close()\n",
    "\n",
    "with open(meta_file, 'w') as f2:\n",
    "    for line in line_list:\n",
    "        to_wrt = line.replace('/', '_') + '.h5' + ' 7' + '\\n'\n",
    "        f2.write(to_wrt)\n",
    "f2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('e2vsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80a5efb8414db1ac8a271b920b54f57b8288712ce1a871f9fa6dcbbadd404fc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
